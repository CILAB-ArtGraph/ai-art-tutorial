{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from onedrivedownloader import download\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from load_dataset.artgraph import ArtGraph\n",
    "from timm import create_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://unibari-my.sharepoint.com/:u:/g/personal/raffaele_scaringi_uniba_it/EQO8aUdMrrFKlaOSfW9a-WQBy7hd9BWcH2-pEWY5nhWrxA?e=u3AKxW'\n",
    "destination_dir = 'data'\n",
    "#download(url=data_url, filename='file.zip', unzip=True, unzip_path=destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load(f'{destination_dir}/dataset/common_dataset/train_data.pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001B[1martwork\u001B[0m={ x=[116475, 128] },\n",
       "  \u001B[1martist\u001B[0m={ x=[2501, 1] },\n",
       "  \u001B[1mgallery\u001B[0m={ x=[1099, 1] },\n",
       "  \u001B[1mcity\u001B[0m={ x=[596, 1] },\n",
       "  \u001B[1mcountry\u001B[0m={ x=[58, 1] },\n",
       "  \u001B[1mstyle\u001B[0m={ x=[32, 1] },\n",
       "  \u001B[1mperiod\u001B[0m={ x=[186, 1] },\n",
       "  \u001B[1mgenre\u001B[0m={ x=[18, 1] },\n",
       "  \u001B[1mserie\u001B[0m={ x=[823, 1] },\n",
       "  \u001B[1mtag\u001B[0m={ x=[5424, 1] },\n",
       "  \u001B[1mmedia\u001B[0m={ x=[167, 1] },\n",
       "  \u001B[1msubject\u001B[0m={ x=[6985, 1] },\n",
       "  \u001B[1mtraining_node\u001B[0m={ x=[268, 1] },\n",
       "  \u001B[1mfield\u001B[0m={ x=[54, 1] },\n",
       "  \u001B[1mmovement\u001B[0m={ x=[243, 1] },\n",
       "  \u001B[1mpeople\u001B[0m={ x=[109, 1] },\n",
       "  \u001B[1memotion\u001B[0m={ x=[9, 1] },\n",
       "  \u001B[1m(artist, belongstofield, field)\u001B[0m={ edge_index=[2, 987] },\n",
       "  \u001B[1m(artist, belongstomovement, movement)\u001B[0m={ edge_index=[2, 1056] },\n",
       "  \u001B[1m(artist, haspatron, people)\u001B[0m={ edge_index=[2, 124] },\n",
       "  \u001B[1m(artist, hassubject, subject)\u001B[0m={ edge_index=[2, 21054] },\n",
       "  \u001B[1m(artist, relatedtoschool, training_node)\u001B[0m={ edge_index=[2, 498] },\n",
       "  \u001B[1m(artist, trainedby, artist)\u001B[0m={ edge_index=[2, 47] },\n",
       "  \u001B[1m(artwork, about, tag)\u001B[0m={ edge_index=[2, 181644] },\n",
       "  \u001B[1m(artwork, createdby, artist)\u001B[0m={ edge_index=[2, 52188] },\n",
       "  \u001B[1m(artwork, elicit, emotion)\u001B[0m={ edge_index=[2, 52188] },\n",
       "  \u001B[1m(artwork, hasgenre, genre)\u001B[0m={ edge_index=[2, 52188] },\n",
       "  \u001B[1m(artwork, hasperiod, period)\u001B[0m={ edge_index=[2, 2870] },\n",
       "  \u001B[1m(artwork, hasstyle, style)\u001B[0m={ edge_index=[2, 52188] },\n",
       "  \u001B[1m(artwork, locatedin, city)\u001B[0m={ edge_index=[2, 11753] },\n",
       "  \u001B[1m(artwork, locatedin, country)\u001B[0m={ edge_index=[2, 11726] },\n",
       "  \u001B[1m(artwork, locatedin, gallery)\u001B[0m={ edge_index=[2, 11736] },\n",
       "  \u001B[1m(artwork, madeof, media)\u001B[0m={ edge_index=[2, 47910] },\n",
       "  \u001B[1m(artwork, partof, serie)\u001B[0m={ edge_index=[2, 3362] },\n",
       "  \u001B[1m(city, incountry, country)\u001B[0m={ edge_index=[2, 608] },\n",
       "  \u001B[1m(gallery, incity, city)\u001B[0m={ edge_index=[2, 1109] },\n",
       "  \u001B[1m(gallery, incountry, country)\u001B[0m={ edge_index=[2, 1105] }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = torch.load(f'{destination_dir}/model/best_new_multitask_model.pt')\n",
    "feature_extractor = torch.load(f'{destination_dir}/model/vit_fine_tune_style.pt')\n",
    "pca = joblib.load(f'{destination_dir}/model/pca.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewMultiTaskClassificationModel(\n",
       "  (encoder_style): MultiGNNEncoder(\n",
       "    (activation): Tanh()\n",
       "    (encoders): ModuleDict(\n",
       "      (0): HeteroConv(num_relations=1)\n",
       "    )\n",
       "  )\n",
       "  (encoder_genre): MultiGNNEncoder(\n",
       "    (activation): Tanh()\n",
       "    (encoders): ModuleDict(\n",
       "      (0): HeteroConv(num_relations=1)\n",
       "    )\n",
       "  )\n",
       "  (encoder_emotion): MultiGNNEncoder(\n",
       "    (activation): Tanh()\n",
       "    (encoders): ModuleDict(\n",
       "      (0): HeteroConv(num_relations=1)\n",
       "    )\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=7680, out_features=3840, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=3840, out_features=1920, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=1920, out_features=960, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=960, out_features=480, bias=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (style_head): Linear(in_features=480, out_features=31, bias=True)\n",
       "  (genre_head): Linear(in_features=480, out_features=18, bias=True)\n",
       "  (emotion_head): Linear(in_features=480, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GELU' object has no attribute 'approximate'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\IPython\\core\\formatters.py:708\u001B[0m, in \u001B[0;36mPlainTextFormatter.__call__\u001B[1;34m(self, obj)\u001B[0m\n\u001B[0;32m    701\u001B[0m stream \u001B[38;5;241m=\u001B[39m StringIO()\n\u001B[0;32m    702\u001B[0m printer \u001B[38;5;241m=\u001B[39m pretty\u001B[38;5;241m.\u001B[39mRepresentationPrinter(stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    703\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnewline,\n\u001B[0;32m    704\u001B[0m     max_seq_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length,\n\u001B[0;32m    705\u001B[0m     singleton_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msingleton_printers,\n\u001B[0;32m    706\u001B[0m     type_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype_printers,\n\u001B[0;32m    707\u001B[0m     deferred_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeferred_printers)\n\u001B[1;32m--> 708\u001B[0m \u001B[43mprinter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    709\u001B[0m printer\u001B[38;5;241m.\u001B[39mflush()\n\u001B[0;32m    710\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\u001B[38;5;241m.\u001B[39mgetvalue()\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\IPython\\lib\\pretty.py:410\u001B[0m, in \u001B[0;36mRepresentationPrinter.pretty\u001B[1;34m(self, obj)\u001B[0m\n\u001B[0;32m    407\u001B[0m                         \u001B[38;5;28;01mreturn\u001B[39;00m meth(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[0;32m    408\u001B[0m                 \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mobject\u001B[39m \\\n\u001B[0;32m    409\u001B[0m                         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__repr__\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[1;32m--> 410\u001B[0m                     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_repr_pprint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcycle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_pprint(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[0;32m    413\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\IPython\\lib\\pretty.py:778\u001B[0m, in \u001B[0;36m_repr_pprint\u001B[1;34m(obj, p, cycle)\u001B[0m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001B[39;00m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;66;03m# Find newlines and replace them with p.break_()\u001B[39;00m\n\u001B[1;32m--> 778\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    779\u001B[0m lines \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39msplitlines()\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgroup():\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2494\u001B[0m, in \u001B[0;36mModule.__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2492\u001B[0m child_lines \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m-> 2494\u001B[0m     mod_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2495\u001B[0m     mod_str \u001B[38;5;241m=\u001B[39m _addindent(mod_str, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m   2496\u001B[0m     child_lines\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m key \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m mod_str)\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2494\u001B[0m, in \u001B[0;36mModule.__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2492\u001B[0m child_lines \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m-> 2494\u001B[0m     mod_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2495\u001B[0m     mod_str \u001B[38;5;241m=\u001B[39m _addindent(mod_str, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m   2496\u001B[0m     child_lines\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m key \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m mod_str)\n",
      "    \u001B[1;31m[... skipping similar frames: Module.__repr__ at line 2494 (1 times)]\u001B[0m\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2494\u001B[0m, in \u001B[0;36mModule.__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2492\u001B[0m child_lines \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_modules\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m-> 2494\u001B[0m     mod_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2495\u001B[0m     mod_str \u001B[38;5;241m=\u001B[39m _addindent(mod_str, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m   2496\u001B[0m     child_lines\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m key \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m mod_str)\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:2488\u001B[0m, in \u001B[0;36mModule.__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2485\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__repr__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   2486\u001B[0m     \u001B[38;5;66;03m# We treat the extra repr like the sub-module, one item per line\u001B[39;00m\n\u001B[0;32m   2487\u001B[0m     extra_lines \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m-> 2488\u001B[0m     extra_repr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextra_repr\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2489\u001B[0m     \u001B[38;5;66;03m# empty string will be split into list ['']\u001B[39;00m\n\u001B[0;32m   2490\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m extra_repr:\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\activation.py:685\u001B[0m, in \u001B[0;36mGELU.extra_repr\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    684\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextra_repr\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m--> 685\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapproximate=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mrepr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapproximate)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[1;32mD:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'GELU' object has no attribute 'approximate'"
     ]
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.reset_classifier(num_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionPipeline():\n",
    "    def __init__(self, model, feature_extractor, pca, graph, preprocessing, mapping_dir):\n",
    "        self.model = model\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.graph = graph\n",
    "        self.preprocessing = preprocessing\n",
    "        self.pca = pca\n",
    "        self.mapping_dir = mapping_dir\n",
    "        \n",
    "    def map_labels(self, style, genre, emotion):\n",
    "        style2name = pd.read_csv(f\"{self.mapping_dir}/style_entidx2name.csv\", names=[\"idx\", \"name\"]).drop(0)\n",
    "        style2name[\"idx\"] -= 1\n",
    "        style2name.reset_index(inplace=True, drop=True)\n",
    "        genre2name = pd.read_csv(f\"{self.mapping_dir}/genre_entidx2name.csv\", names=[\"idx\", \"name\"])\n",
    "        emotion2name = pd.read_csv(f\"{self.mapping_dir}/emotion_entidx2name.csv\", names=[\"idx\", \"name\"])\n",
    "        return style2name.loc[style, \"name\"], genre2name.loc[genre, \"name\"], emotion2name.loc[emotion, \"name\"]\n",
    "        \n",
    "    def predict(self, image):\n",
    "        tensor = self.preprocessing(image.convert('RGB')).unsqueeze(dim=0).to('cuda')\n",
    "        features = self.pca.transform(self.feature_extractor(tensor).cpu().numpy())\n",
    "        ans = self.model(torch.from_numpy(features).to('cuda'), self.graph.x_dict, self.graph.edge_index_dict)\n",
    "        style, genre, emotion = ans\n",
    "        style, genre, emotion = torch.argmax(style).item(), torch.argmax(genre).item(), torch.argmax(emotion).item()\n",
    "        style, genre, emotion = self.map_labels(style, genre, emotion)\n",
    "        return f\"The artwork is in {style} style, represents a {genre}, and can evoke {emotion}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = Compose([\n",
    "    ToTensor(),\n",
    "    Resize((224, 224)),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "pipeline = PredictionPipeline(model,\n",
    "                              feature_extractor,\n",
    "                              pca,\n",
    "                              data,\n",
    "                              preprocessing,\n",
    "                              f\"{destination_dir}/dataset/artgraph2bestemotions/mapping\"\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\gradio\\routes.py\", line 544, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\gradio\\route_utils.py\", line 217, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\gradio\\blocks.py\", line 1553, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\gradio\\blocks.py\", line 1191, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\gradio\\utils.py\", line 659, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_5272\\4270139438.py\", line 20, in predict\n",
      "    features = self.pca.transform(self.feature_extractor(tensor).cpu().numpy())\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\timm\\models\\vision_transformer.py\", line 465, in forward\n",
      "    x = self.forward_features(x)\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\timm\\models\\vision_transformer.py\", line 450, in forward_features\n",
      "    x = self._pos_embed(x)\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\timm\\models\\vision_transformer.py\", line 434, in _pos_embed\n",
      "    if self.no_embed_class:\n",
      "  File \"D:\\repo_git\\ai-art-tutorial\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'VisionTransformer' object has no attribute 'no_embed_class'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "gr.Interface(\n",
    "    pipeline.predict,\n",
    "    title=\"Artwork Attribtue Recognition with KGs\",\n",
    "    inputs=[\n",
    "        gr.Image(source='upload', type='pil')\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox()\n",
    "    ]\n",
    ").launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
